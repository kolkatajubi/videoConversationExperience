{
  "_args": [
    [
      {
        "raw": "wink-tokenizer@^5.2.1",
        "scope": null,
        "escapedName": "wink-tokenizer",
        "name": "wink-tokenizer",
        "rawSpec": "^5.2.1",
        "spec": ">=5.2.1 <6.0.0",
        "type": "range"
      },
      "C:\\Users\\suvo\\Desktop\\videoConversationExperience\\node_modules\\wink-nlp-utils"
    ]
  ],
  "_from": "wink-tokenizer@^5.2.1",
  "_hasShrinkwrap": false,
  "_id": "wink-tokenizer@5.2.1",
  "_location": "/wink-tokenizer",
  "_nodeVersion": "10.15.0",
  "_npmOperationalInternal": {
    "host": "s3://npm-registry-packages",
    "tmp": "tmp/wink-tokenizer_5.2.1_1555402897662_0.09503949709734294"
  },
  "_npmUser": {
    "name": "sanjaya",
    "email": "sanjaya@graype.in"
  },
  "_npmVersion": "6.9.0",
  "_phantomChildren": {},
  "_requested": {
    "raw": "wink-tokenizer@^5.2.1",
    "scope": null,
    "escapedName": "wink-tokenizer",
    "name": "wink-tokenizer",
    "rawSpec": "^5.2.1",
    "spec": ">=5.2.1 <6.0.0",
    "type": "range"
  },
  "_requiredBy": [
    "/wink-nlp-utils"
  ],
  "_resolved": "https://registry.npmjs.org/wink-tokenizer/-/wink-tokenizer-5.2.1.tgz",
  "_shasum": "3c53fb6fb18dbf75b7da69f1808c0a447721963f",
  "_shrinkwrap": null,
  "_spec": "wink-tokenizer@^5.2.1",
  "_where": "C:\\Users\\suvo\\Desktop\\videoConversationExperience\\node_modules\\wink-nlp-utils",
  "author": {
    "name": "Sanjaya Kumar Saxena"
  },
  "bugs": {
    "url": "https://github.com/winkjs/wink-tokenizer/issues"
  },
  "dependencies": {},
  "description": "Multilingual tokenizer that automatically tags each token with its type",
  "devDependencies": {
    "chai": "^4.2.0",
    "coveralls": "^3.0.3",
    "docdash": "github:winkjs/docdash",
    "docker": "^1.0.0",
    "eslint": "^5.16.0",
    "istanbul": "^0.4.5",
    "jsdoc": "^3.5.5",
    "mocha": "^6.1.3",
    "mocha-lcov-reporter": "^1.3.0"
  },
  "directories": {},
  "dist": {
    "integrity": "sha512-8WwUToAUvioN6zYCAgtgmqLZEgkLs9whJMVsd/l0NbWynRCz2HreP/YF9OzRS48H4kLIQCP0//ZvGw1cwVz2sg==",
    "shasum": "3c53fb6fb18dbf75b7da69f1808c0a447721963f",
    "tarball": "https://registry.npmjs.org/wink-tokenizer/-/wink-tokenizer-5.2.1.tgz",
    "fileCount": 9,
    "unpackedSize": 68752,
    "npm-signature": "-----BEGIN PGP SIGNATURE-----\r\nVersion: OpenPGP.js v3.0.4\r\nComment: https://openpgpjs.org\r\n\r\nwsFcBAEBCAAQBQJctZCSCRA9TVsSAnZWagAAR4EP/2+5hRDALki+/O0Mokxt\nNOCc9jCdGRHycTib49PZPHsNwGGTHfwU5KJdChWY0bza/cNGNTMk/3EpXHiq\nKl+K61TX3dA80K5fbtlzlmvVhyZvzRu2DYiwzXBIengyVBY6a+1vDnp9cm1Q\nVA9avzWDs/adv6lOcIH27d8hfksPU7khTsSwWebgZV9lFil8tZH2PeSld3I5\n9R27DcGOtLYEwLtINUKx1sIulpMrUPsTswmkV9oOdOxf5g8nUVhXUT//OhJE\n+JKzMOK9O1VHN0wamyGCYSTXDFzUwTQTK5hosOVMMrexC5qe5VnlHnewMer+\nEGX5FVtag4WlCGqO2edHKw8j9Eq/anA+SSq4hJAxoFVi9CmM3LU9T+6i5sKx\nPE/1S9CBOSPqm1YJzA5lS/yVm6CLSYEmpWyRtxN6ofHCDqUx92C7UBxVjTAn\np2w9MpGw1oKsWPmvFtfYf37yK+LwejJcDNgZYhTwXVRnd63vN5mzKNq2CUdn\nKnRT+zJYPUNbhgA2azNbRpTnWZ7VwYG9SOPxIvXf6kSZnnch7lr7COu1Xu8Y\nR2s9EmC4GUtA62zVoKuiuyArupjjB3Qh2CzI90GPFZNgwHi6Au87JCowwt15\nNv7bZScDXEivQmKJd31at+ZXT3/vZqzvZU1KoCUbpvf87zQERCHDvmfUsD6S\n62rs\r\n=jVXL\r\n-----END PGP SIGNATURE-----\r\n"
  },
  "gitHead": "b7d348a2a7d56055d28ce7fcb0648989a2d0a091",
  "homepage": "http://winkjs.org/",
  "keywords": [
    "Tokenizer",
    "Tokenize",
    "Tags",
    "Tagging",
    "NLP",
    "email",
    "twitter",
    "URL",
    "Emoji",
    "Emoticon",
    "Multilingual",
    "French",
    "German",
    "Spanish",
    "Icelandic",
    "wink"
  ],
  "license": "MIT",
  "main": "src/wink-tokenizer.js",
  "maintainers": [
    {
      "name": "prtksxna",
      "email": "prtksxna@gmail.com"
    },
    {
      "name": "r4chn4",
      "email": "rachna@graype.in"
    },
    {
      "name": "sanjaya",
      "email": "sanjaya@graype.in"
    }
  ],
  "name": "wink-tokenizer",
  "optionalDependencies": {},
  "readme": "# wink-tokenizer\n\nMultilingual tokenizer that automatically tags each token with its type\n\n### [![Build Status](https://api.travis-ci.org/winkjs/wink-tokenizer.svg?branch=master)](https://travis-ci.org/winkjs/wink-tokenizer) [![Coverage Status](https://coveralls.io/repos/github/winkjs/wink-tokenizer/badge.svg?branch=master)](https://coveralls.io/github/winkjs/wink-tokenizer?branch=master) [![Inline docs](http://inch-ci.org/github/winkjs/wink-tokenizer.svg?branch=master)](http://inch-ci.org/github/winkjs/wink-tokenizer) [![devDependencies Status](https://david-dm.org/winkjs/wink-tokenizer/dev-status.svg)](https://david-dm.org/winkjs/wink-tokenizer?type=dev) [![Gitter](https://img.shields.io/gitter/room/nwjs/nw.js.svg)](https://gitter.im/winkjs/Lobby)\n\n[<img align=\"right\" src=\"https://decisively.github.io/wink-logos/logo-title.png\" width=\"100px\" >](http://winkjs.org/)\n\nTokenize sentences in Latin and Devanagari scripts using **`wink-tokenizer`**. Some of it's top feature are outlined below:\n\n1. Support for English, French, German, Hindi, Sanskrit, Marathi and many more.\n\n1. Intelligent tokenization of sentence containing words in more than one language.\n\n1. Automatic detection & tagging of different types of tokens based on their features:\n     - These include word, punctuation, email, mention, hashtag, emoticon, and emoji etc.\n     - User definable token types.\n\n1. High performance ‚Äì tokenizes a typical english sentence at speed of over **2.4 million tokens/second** and a complex tweet containing hashtags, emoticons, emojis, mentions, e-mail at a speed of over **1.5 million tokens/second** (benchmarked on 2.2 GHz Intel Core i7 machine with 16GB RAM).\n\n\n### Installation\n\nUse [npm](https://www.npmjs.com/package/wink-tokenizer) to install:\n\n    npm install wink-tokenizer --save\n\n### Getting Started\n```javascript\n// Load tokenizer.\nvar tokenizer = require( 'wink-tokenizer' );\n// Create it's instance.\nvar myTokenizer = tokenizer();\n\n// Tokenize a tweet.\nvar s = '@superman: hit me up on my email r2d2@gmail.com, 2 of us plan partyüéâ tom at 3pm:) #fun';\nmyTokenizer.tokenize( s );\n// -> [ { value: '@superman', tag: 'mention' },\n//      { value: ':', tag: 'punctuation' },\n//      { value: 'hit', tag: 'word' },\n//      { value: 'me', tag: 'word' },\n//      { value: 'up', tag: 'word' },\n//      { value: 'on', tag: 'word' },\n//      { value: 'my', tag: 'word' },\n//      { value: 'email', tag: 'word' },\n//      { value: 'r2d2@gmail.com', tag: 'email' },\n//      { value: ',', tag: 'punctuation' },\n//      { value: '2', tag: 'number' },\n//      { value: 'of', tag: 'word' },\n//      { value: 'us', tag: 'word' },\n//      { value: 'plan', tag: 'word' },\n//      { value: 'party', tag: 'word' },\n//      { value: 'üéâ', tag: 'emoji' },\n//      { value: 'tom', tag: 'word' },\n//      { value: 'at', tag: 'word' },\n//      { value: '3pm', tag: 'time' },\n//      { value: ':)', tag: 'emoticon' },\n//      { value: '#fun', tag: 'hashtag' } ]\n\n// Tokenize a French sentence.\ns = 'Mieux vaut pr√©venir que gu√©rir:-)';\nmyTokenizer.tokenize( s );\n// -> [ { value: 'Mieux', tag: 'word' },\n//      { value: 'vaut', tag: 'word' },\n//      { value: 'pr√©venir', tag: 'word' },\n//      { value: 'que', tag: 'word' },\n//      { value: 'gu√©rir', tag: 'word' },\n//      { value: ':-)', tag: 'emoticon' } ]\n\n// Tokenize a sentence containing Hindi and English.\ns = '‡§¶‡•ç‡§∞‡§µ‡§ø‡§°‡§º ‡§®‡•á ‡§ü‡•á‡§∏‡•ç‡§ü ‡§Æ‡•á‡§Ç ‡•©‡•¨ ‡§∂‡§§‡§ï ‡§ú‡§Æ‡§æ‡§è, ‡§â‡§®‡§Æ‡•á‡§Ç 21 ‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä playground ‡§™‡§∞ ‡§π‡•à‡§Ç‡•§';\nmyTokenizer.tokenize( s );\n// -> [ { value: '‡§¶‡•ç‡§∞‡§µ‡§ø‡§°‡§º', tag: 'word' },\n//      { value: '‡§®‡•á', tag: 'word' },\n//      { value: '‡§ü‡•á‡§∏‡•ç‡§ü', tag: 'word' },\n//      { value: '‡§Æ‡•á‡§Ç', tag: 'word' },\n//      { value: '‡•©‡•¨', tag: 'number' },\n//      { value: '‡§∂‡§§‡§ï', tag: 'word' },\n//      { value: '‡§ú‡§Æ‡§æ‡§è', tag: 'word' },\n//      { value: ',', tag: 'punctuation' },\n//      { value: '‡§â‡§®‡§Æ‡•á‡§Ç', tag: 'word' },\n//      { value: '21', tag: 'number' },\n//      { value: '‡§µ‡§ø‡§¶‡•á‡§∂‡•Ä', tag: 'word' },\n//      { value: 'playground', tag: 'word' },\n//      { value: '‡§™‡§∞', tag: 'word' },\n//      { value: '‡§π‡•à‡§Ç', tag: 'word' },\n//      { value: '‡•§', tag: 'punctuation' } ]\n```\n\n### Documentation\nCheck out the [tokenizer API documentation](http://winkjs.org/wink-tokenizer/) to learn more.\n\n### Need Help?\n\nIf you spot a bug and the same has not yet been reported, raise a new [issue](https://github.com/winkjs/wink-tokenizer/issues) or consider fixing it and sending a pull request.\n\n### About wink\n[Wink](http://winkjs.org/) is a family of open source packages for **Statistical Analysis**, **Natural Language Processing** and **Machine Learning** in NodeJS. The code is **thoroughly documented** for easy human comprehension and has a **test coverage of ~100%** for reliability to build production grade solutions.\n\n\n### Copyright & License\n\n**wink-tokenizer** is copyright 2017-19 [GRAYPE Systems Private Limited](http://graype.in/).\n\nIt is licensed under the terms of the MIT License.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git+https://github.com/winkjs/wink-tokenizer.git"
  },
  "runkitExampleFilename": "./runkit/example.js",
  "scripts": {
    "coveralls": "istanbul cover _mocha --report lcovonly -- -R spec && cat ./coverage/lcov.info | coveralls && rm -rf ./coverage",
    "docs": "jsdoc src/*.js -c .jsdoc.json",
    "lint": "eslint ./src/*.js ./test/*.js ./runkit/*.js",
    "pretest": "npm run lint && npm run docs",
    "sourcedocs": "docker -i src -o ./sourcedocs --sidebar no",
    "test": "istanbul cover _mocha ./test/"
  },
  "version": "5.2.1"
}
